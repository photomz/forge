# Config for supervised full finetuning using a Llama3.1 8B Instruct model
#
# This config assumes that you've run the following command before launching
# this run:
#   export HF_HUB_DISABLE_XET=1
#   uv run forge download meta-llama/Meta-Llama-3.1-8B-Instruct


# TODO: required by torchtitan
# https://github.com/pytorch/torchtitan/blob/2f1c814da071cc8ad165d00be6f9c1a66f8e1cce/torchtitan/distributed/utils.py#L265
comm:
  trace_buf_size: 0

model:
  name: llama3
  flavor: 8B
  tokenizer_path: /tmp/Llama-3.1-8B-Instruct

processes:
  scheduler: local # local | mast (not supported yet)
  num_hosts: 1
  num_procs: 8
  with_gpus: true

optimizer:
  name: AdamW
  lr: 1e-5
  eps: 1e-8

lr_scheduler:
  warmup_steps: 200

training:
  local_batch_size: 1
  seq_len: 2048
  max_norm: 1.0
  steps: 1000
  compile: false
  dataset: "c4"

parallelism:
  data_parallel_replicate_degree: 1
  data_parallel_shard_degree: -1
  tensor_parallel_degree: 1
  pipeline_parallel_degree: 1
  context_parallel_degree: 1
  expert_parallel_degree: 1
  disable_loss_parallel: false

checkpoint:
  enable: true
  folder: /tmp/Meta-Llama-3.1-8B-Instruct/saved_checkpoints
  initial_load_path:  /tmp/Meta-Llama-3.1-8B-Instruct/
  initial_load_in_hf: true
  last_save_in_hf: true
  interval: 500
  async_mode: "disabled"

activation_checkpoint:
  mode: selective
  selective_ac_option: op

# profiling:
#   enable_profiling: false

# metrics:
#   log_freq: 10
#   enable_tensorboard: true
#   save_tb_folder: "tb"
