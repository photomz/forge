# Grouped Relative Policy Optimization (GRPO)
# >>> python -m apps.grpo.main --config apps/grpo/qwen3_1_7b.yaml

# Global configuration
group_size: 8
local_batch_size: 16 # per-device batch size
max_req_tokens: 1024
max_res_tokens: 1024
model: "Qwen/Qwen3-1.7B"
off_by_n: 1 # Off by one by default

# Main loop configuration
rollout_threads: 1   # Recommended to set equal to policy.num_replicas


# Observability configuration
metric_logging:
  wandb:
    project: grpo-training
    group: grpo_exp_${oc.env:USER}
    logging_mode: global_reduce # global_reduce, per_rank_reduce, per_rank_no_reduce
    mode: offline  # Run wandb in offline mode (no API key needed)
  console:
    logging_mode: global_reduce

# Dataset configuration
dataset:
  path: "openai/gsm8k"
  revision: "main"
  data_split: "train"
  streaming: true
  model: ${model}

# Policy configuration
policy:
  engine_args:  # https://docs.vllm.ai/en/v0.10.0/api/vllm/engine/arg_utils.html#vllm.engine.arg_utils.EngineArgs
    model: ${model}
    tensor_parallel_size: 2  # Each replica: 2 GPUs with TP=2 (each GPU = 1/2 weights)
    pipeline_parallel_size: 1
    enforce_eager: false
    # Set weight quantization method (ensure ${model} points to a matching quantized repo)
    # Supported values include: awq, gptq, marlin, squeezellm (depends on vLLM version)
    quantization: awq
  sampling_params:  # https://docs.vllm.ai/en/v0.10.0/api/vllm/sampling_params.html#vllm.sampling_params.SamplingParams
    n: ${group_size}
    max_tokens: ${max_res_tokens}
    temperature: 1.0
    top_p: 1.0

# Trainer configuration
trainer:
  model:
    name: qwen3
    flavor: 1.7B
    hf_assets_path: hf://${model}
  optimizer:
    name: AdamW
    lr: 1e-5
    eps: 1e-8
  lr_scheduler:
    warmup_steps: 1
  training:
    local_batch_size: ${local_batch_size}
    seq_len: 2052  # Must be >= max_req_tokens + max_res_tokens AND divisible by TP * 2 * CP = 6
    max_norm: 1.0
    steps: 1000000
    dtype: bfloat16
    gc_freq: 1
  compile:
    enable: false
  parallelism:
    data_parallel_replicate_degree: 1
    data_parallel_shard_degree: 1    # No data parallel sharding
    tensor_parallel_degree: 2        # TP across 2 GPUs (each GPU = 1/2 weights)
    pipeline_parallel_degree: 1
    context_parallel_degree: 1
    expert_parallel_degree: 1
    disable_loss_parallel: true
  checkpoint:
    enable: true
    folder: ./checkpoint              # The folder to save checkpoints to.
    initial_load_path: hf://${model}  # The path to load the initial checkpoint from. Ignored if `folder` exists.
    initial_load_in_hf: true          # If true, interpret initial_load_path as a HuggingFace model repo
    last_save_in_hf: true
    interval: 500
    async_mode: "disabled"
  activation_checkpoint:
    mode: selective
    selective_ac_option: op

# Replay buffer configuration
replay_buffer:
  batch_size: ${local_batch_size}
  max_policy_age: ${off_by_n}
  dp_size: 1  # Must equal trainer DP degree (no data parallelism, only TP)

# Reference model configuration
ref_model:
  model:
    name: qwen3
    flavor: 1.7B
    hf_assets_path: hf://${model}
  training:
    seq_len: ${trainer.training.seq_len}
    dtype: bfloat16
    gc_freq: 1
  compile:
    enable: false
  parallelism:
    data_parallel_replicate_degree: 1
    data_parallel_shard_degree: 1
    tensor_parallel_degree: 1
    pipeline_parallel_degree: 1
    context_parallel_degree: 1
    expert_parallel_degree: 1
  checkpoint:
    enable: true
    initial_load_path: hf://${model}
    initial_load_in_hf: true

# All resource allocations
services:
  policy:
    procs: ${policy.engine_args.tensor_parallel_size}  # 2 GPUs per replica (TP=2)
    num_replicas: 2  # 2 replicas (total: 2 replicas × 2 GPUs = 4 GPUs)
    mesh_name: policy
    with_gpus: true
  ref_model:
    procs: 1 # markus -- ignore but too lazy to remove.
    num_replicas: 1
    mesh_name: ref_model
    with_gpus: true
  reward_actor:
    procs: 1
    num_replicas: 1
    mesh_name: reward_actor
    with_gpus: false

actors:
  dataset:
    procs: 1
    with_gpus: false
    mesh_name: dataset
  trainer:
    procs: 2  # 2 GPUs with TP=2 (each GPU holds 1/2 of weights)
    with_gpus: true
    mesh_name: trainer
  replay_buffer:
    procs: 1
    with_gpus: false
    mesh_name: replay_buffer
  compute_advantages:
    procs: 1
    with_gpus: false
    mesh_name: compute_advantages


# ┌─────────────────────────────────────────────────────────────┐
# │                     GPU Allocation (8 GPUs)                  │
# └─────────────────────────────────────────────────────────────┘

# POLICY (4 GPUs - vLLM with TP + Replicas)
# ┌──────────────────────────┐  ┌──────────────────────────┐
# │   Replica 0 (2 GPUs)     │  │   Replica 1 (2 GPUs)     │
# │  ┌────────┬────────┐     │  │  ┌────────┬────────┐     │
# │  │ GPU 0  │ GPU 1  │     │  │  │ GPU 2  │ GPU 3  │     │
# │  │  1/2   │  1/2   │     │  │  │  1/2   │  1/2   │     │
# │  │weights │weights │     │  │  │weights │weights │     │
# │  └────────┴────────┘     │  │  └────────┴────────┘     │
# │    (Tensor Parallel)     │  │    (Tensor Parallel)     │
# └──────────────────────────┘  └──────────────────────────┘
#        ↓ generates text            ↓ generates text
#     (Load balanced - both replicas serve requests)

# TRAINER (3 GPUs - TorchTitan with TP)
# ┌──────────────────────────────────────┐
# │         Single Replica (3 GPUs)       │
# │  ┌────────┬────────┬────────┐        │
# │  │ GPU 4  │ GPU 5  │ GPU 6  │        │
# │  │  1/3   │  1/3   │  1/3   │        │
# │  │weights │weights │weights │        │
# │  └────────┴────────┴────────┘        │
# │       (Tensor Parallel)               │
# └──────────────────────────────────────┘
#        ↓ trains model with gradients

# REF MODEL (1 GPU)
# ┌──────────┐
# │  GPU 7   │  (ref_model.with_gpus: true)
# └──────────┘