# GRPO Training Configuration
# Currently a fork of the main yaml, this just shows
# placement of trainer and inference servers on separate hosts.

# Global configuration
group_size: 8
batch_size: 16
max_req_tokens: 512
max_res_tokens: 512
model: "Qwen/Qwen3-1.7B"

# Observability configuration
metric_logging:
  wandb:
    project: "grpo-training"
    group: "grpo_exp_${oc.env:USER}"
    reduce_across_ranks: True
  console:
    reduce_across_ranks: True

# Dataset configuration
dataset:
  path: "openai/gsm8k"
  revision: "main"
  data_split: "train"
  streaming: true
  model: ${model}

# Policy configuration
policy:
  engine_config:
    model: ${model}
    tensor_parallel_size: 1
    pipeline_parallel_size: 1
    enforce_eager: false
  sampling_config:
    n: ${group_size}
    max_tokens: ${max_res_tokens}
    temperature: 1.0
    top_p: 1.0

# Trainer configuration
trainer:
  model_name: ${model}
  learning_rate: 1e-5

# Replay buffer configuration
replay_buffer:
  batch_size: ${batch_size}
  max_policy_age: 1 # Async by 1
  dp_size: 1

# Reference model configuration
ref_model:
  model_name: ${model}

services:
  policy:
    procs: 1
    hosts: 1
    num_replicas: 1
    with_gpus: true
  ref_model:
    procs: 1
    num_replicas: 1
    with_gpus: true
  reward_actor:
    procs: 1
    num_replicas: 1
    with_gpus: false

actors:
  dataset:
    procs: 1
    with_gpus: false
  compute_advantages:
    procs: 1
    with_gpus: false
  trainer:
    procs: 1
    hosts: 1
    with_gpus: true
  replay_buffer:
    procs: 1
    with_gpus: false
