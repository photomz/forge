# >>> python -m tests.sandbox.vllm.main --config tests/sandbox/vllm/llama3_8b.yaml

policy:
  engine_args:
    model: "meta-llama/Llama-3.1-8B-Instruct"
    tensor_parallel_size: 2
    pipeline_parallel_size: 1
    enforce_eager: true
  sampling_params:
    n: 2
    max_tokens: 512

services:
  policy:
    procs: ${policy.engine_args.tensor_parallel_size}
    num_replicas: 4
    with_gpus: true


# Optional, otherwise argparse fallback kicks in
prompt: "Tell me a joke"
